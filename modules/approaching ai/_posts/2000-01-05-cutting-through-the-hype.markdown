---
title: Cutting Through the Hype
---

# Cutting Through the Hype

<br>
Media outlets continue to write stories about AI -- and today especially, deep neural networks -- that are grotesquely sensationalized, often to the point of <b>anthropomorphizing</b> or attributing human-like characteristics to computer programs.

<br>
For example, consider the headline <i>Google Supercomputer Gives Birth To Its Own AI Child.</i> (This is a real headline; if you <a href='https://www.google.com/search?q=Google+Supercomputer+Gives+Birth+To+Its+Own+AI+Child'>Google it</a> you'll find plenty of articles with similar headlines.)

<br>
<center><img src="{{ site.baseurl }}/img/google-supercomputer-gives-birth-to-its-own-ai-child.PNG" width="80%"></center>

<br>
The way the headline is phrased, it would lead you to think that Google researchers built a futuristic computer, and that computer somehow gave birth to a human-like child that is composed of circuits rather than flesh and blood. 

<br>
<center>
  <img src="{{ site.baseurl }}/img/google-supercomputer-creates-its-own-ai-child-dailystar.PNG" width="80%">
  <br>
  <small>Taken from the Daily Star article <a href='https://www.dailystar.co.uk/news/latest-news/664713/google-artificial-intelligence-ai-child-NASNet-AutoML'><i>Google supercomputer creates its own AI CHILD</i></a>.</small>
</center>

<br>
But this is simply not true. Rather, Google researchers built a model to predict a good <b>architecture</b>, or connectivity arrangement, for another model which was meant to perform a specific task. Usually, architectures are chosen by researchers building a model, but in the case of this article, researchers built another model to predict which architecture would lead to good performance on a specific task.

<center>
  <img src="{{ site.baseurl }}/img/cnn-architecture.png" width="80%">
  <br>
  <small>A diagram of a sample model architecture for a convolutional neural network.</small>
</center>

<br>
In some analogy, then, the model being trained to perform the specific task is like a “child,” and the model predicting a good architecture for the child model is like a “parent.” But did this parent model give birth to the child model in a human-like way? Does the child model resemble a human-like child? The answer to both of these questions is a resounding no. 

<br>
If someone didn't realize that the headline <i>Google Supercomputer Gives Birth To Its Own AI Child</i> was just a loose analogy, then they might begin to develop ridiculous opinions about AI based on the false assumption that computers can literally give birth to AI children resembling human children. For example, an uninformed pessimistic reader might argue that we need to stop AI research because a growing population of AI children will pose a threat to the human population. Or, an uninformed optimistic reader might argue that we must rush to pass laws supporting the equality of AI children, lest they be discriminated against. Both arguments are irrelevant to the present state of AI.

<br>
The takeaway of this discussion is that when you read about AI in the news, you should be skeptical of titles which appear sensationalized or anthropomorphized. If some statement about the field of AI is too good (or too bad) to be true, then it probably isn't true in a literal sense. This will become increasingly apparent as you learn more about the technical underpinnings of AI.
